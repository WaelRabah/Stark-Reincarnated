{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install kaggle","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:46:18.251623Z","iopub.execute_input":"2021-10-17T16:46:18.252221Z","iopub.status.idle":"2021-10-17T16:46:25.846363Z","shell.execute_reply.started":"2021-10-17T16:46:18.252183Z","shell.execute_reply":"2021-10-17T16:46:25.845495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['KAGGLE_USERNAME']=\"Username\"\nos.environ['KAGGLE_KEY']=\"Key\"","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:46:25.850166Z","iopub.execute_input":"2021-10-17T16:46:25.850692Z","iopub.status.idle":"2021-10-17T16:46:25.857497Z","shell.execute_reply.started":"2021-10-17T16:46:25.850659Z","shell.execute_reply":"2021-10-17T16:46:25.85669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dialogues():\n    !kaggle datasets download -d pdunton/marvel-cinematic-universe-dialogue\n    !unzip -o marvel-cinematic-universe-dialogue.zip\n    df=pd.read_csv('./mcu.csv')\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-17T16:46:25.859237Z","iopub.execute_input":"2021-10-17T16:46:25.859507Z","iopub.status.idle":"2021-10-17T16:46:25.868106Z","shell.execute_reply.started":"2021-10-17T16:46:25.85948Z","shell.execute_reply":"2021-10-17T16:46:25.867272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -q install transformers contractions unidecode","metadata":{"id":"4BdaeENt2wB3","outputId":"df7be838-8ad2-4e8d-8bb6-847d93f02e23","execution":{"iopub.status.busy":"2021-10-17T16:46:25.870575Z","iopub.execute_input":"2021-10-17T16:46:25.870991Z","iopub.status.idle":"2021-10-17T16:46:33.010822Z","shell.execute_reply.started":"2021-10-17T16:46:25.870872Z","shell.execute_reply":"2021-10-17T16:46:33.009971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp drive/MyDrive/mcu_dialogues.csv .","metadata":{"id":"VBh7xQE-7y-U","execution":{"iopub.status.busy":"2021-10-17T16:46:33.0142Z","iopub.execute_input":"2021-10-17T16:46:33.014448Z","iopub.status.idle":"2021-10-17T16:46:33.884345Z","shell.execute_reply.started":"2021-10-17T16:46:33.014422Z","shell.execute_reply":"2021-10-17T16:46:33.883132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelWithLMHead, AutoTokenizer\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\nmodel = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")","metadata":{"id":"2ZWJ0-W9xelH","outputId":"48cf5924-f331-42d3-ffe9-d86cdf847aa7","execution":{"iopub.status.busy":"2021-10-17T16:46:33.891206Z","iopub.execute_input":"2021-10-17T16:46:33.891505Z","iopub.status.idle":"2021-10-17T16:46:46.507726Z","shell.execute_reply.started":"2021-10-17T16:46:33.891477Z","shell.execute_reply":"2021-10-17T16:46:46.506991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nFine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\nGPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\nusing a masked language modeling (MLM) loss.\n\"\"\"\n\nimport glob\nimport logging\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\nfrom typing import Dict, List, Tuple\n\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom sklearn.model_selection import train_test_split\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm.notebook import tqdm, trange\n\nfrom pathlib import Path\n\nfrom transformers import (\n    MODEL_WITH_LM_HEAD_MAPPING,\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModelWithLMHead,\n    AutoTokenizer,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    get_linear_schedule_with_warmup,\n)\n\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    from tensorboardX import SummaryWriter\n\n# Configs\nlogger = logging.getLogger(__name__)\n\nMODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","metadata":{"id":"h9zuadaSzAZt","execution":{"iopub.status.busy":"2021-10-17T16:46:46.509376Z","iopub.execute_input":"2021-10-17T16:46:46.509632Z","iopub.status.idle":"2021-10-17T16:46:46.520206Z","shell.execute_reply.started":"2021-10-17T16:46:46.509601Z","shell.execute_reply":"2021-10-17T16:46:46.519554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import RegexpTokenizer\nimport contractions\nimport spacy\nfrom nltk.stem import WordNetLemmatizer\nimport unidecode\nimport re\nnlp = spacy.load('en_core_web_sm')\nnltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"id":"1X2rKmRB2kPV","outputId":"00b1ba43-d4cc-43c4-bb5c-ceefc24ea317","execution":{"iopub.status.busy":"2021-10-17T16:46:46.521518Z","iopub.execute_input":"2021-10-17T16:46:46.521915Z","iopub.status.idle":"2021-10-17T16:46:47.416823Z","shell.execute_reply.started":"2021-10-17T16:46:46.52188Z","shell.execute_reply":"2021-10-17T16:46:47.416048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _removeNonAscii( s):\n    return \"\".join(i for i in s if ord(i) < 128)\n\ndef make_lower_case( text):\n    return text.lower()\n\ndef remove_stop_words( text):\n    from nltk.corpus import stopwords\n    text = text.split()\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops]\n    text = \" \".join(text)\n    return text\n\ndef expand_contractions( text):\n    \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n    text = contractions.fix(text)\n    return text\n\ndef lemmatize( text):\n    lemmatizer = WordNetLemmatizer()\n    mytokens = []\n    for word in text.split(\" \"):\n        mytokens.append(lemmatizer.lemmatize(word))\n    return \" \".join(mytokens)\n\ndef remove_accented_chars( text):\n    \"\"\"remove accented characters from text, e.g. cafÃ©\"\"\"\n    text = unidecode.unidecode(text)\n    return text\n\ndef remove_html( text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\ndef remove_numbers( text):\n    return re.sub(r'\\d+', ' ', text)\n\ndef remove_punctuation( text):\n    tokenizer1 = RegexpTokenizer(r'\\w+')\n    text = tokenizer1.tokenize(text)\n    text = \" \".join(text)\n    return text","metadata":{"id":"PNBfWY072niM","execution":{"iopub.status.busy":"2021-10-17T16:46:47.417995Z","iopub.execute_input":"2021-10-17T16:46:47.418659Z","iopub.status.idle":"2021-10-17T16:46:47.42923Z","shell.execute_reply.started":"2021-10-17T16:46:47.418619Z","shell.execute_reply":"2021-10-17T16:46:47.428408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Args():\n    def __init__(self):\n        self.output_dir = 'output-small'\n        self.model_type = 'gpt2'\n        self.model_name_or_path = 'microsoft/DialoGPT-small'\n        self.config_name = 'microsoft/DialoGPT-small'\n        self.tokenizer_name = 'microsoft/DialoGPT-small'\n        self.cache_dir = 'cached'\n        self.block_size = 512\n        self.do_train = True\n        self.do_eval = True\n        self.evaluate_during_training = False\n        self.per_gpu_train_batch_size = 4\n        self.per_gpu_eval_batch_size = 4\n        self.gradient_accumulation_steps = 1\n        self.learning_rate = 1e-5\n        self.weight_decay = 0.0\n        self.adam_epsilon = 1e-8\n        self.max_grad_norm = 1.0\n        self.num_train_epochs = 5\n        self.max_steps = -1\n        self.warmup_steps = 0\n        self.logging_steps = 1000\n        self.save_steps = 3500\n        self.save_total_limit = None\n        self.eval_all_checkpoints = False\n        self.no_cuda = False\n        self.overwrite_output_dir = True\n        self.overwrite_cache = True\n        self.should_continue = False\n        self.seed = 42\n        self.local_rank = -1\n        self.fp16 = False\n        self.fp16_opt_level = 'O1'\n\nargs = Args()","metadata":{"id":"ubMin4ZzzANe","execution":{"iopub.status.busy":"2021-10-17T16:46:47.432682Z","iopub.execute_input":"2021-10-17T16:46:47.433013Z","iopub.status.idle":"2021-10-17T16:46:47.444061Z","shell.execute_reply.started":"2021-10-17T16:46:47.432969Z","shell.execute_reply":"2021-10-17T16:46:47.443352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"id":"5ZD8Ljqw8azf","outputId":"00c0d944-0dce-4235-ea2e-9c8598a4ad84","execution":{"iopub.status.busy":"2021-10-17T16:46:47.445393Z","iopub.execute_input":"2021-10-17T16:46:47.44572Z","iopub.status.idle":"2021-10-17T16:46:48.213314Z","shell.execute_reply.started":"2021-10-17T16:46:47.445689Z","shell.execute_reply":"2021-10-17T16:46:48.212419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fetch data\ndf=get_dialogues()\ndf=df.drop(columns=['Unnamed: 0'])","metadata":{"id":"E53A-KGz1N4b","execution":{"iopub.status.busy":"2021-10-17T16:46:48.216587Z","iopub.execute_input":"2021-10-17T16:46:48.216901Z","iopub.status.idle":"2021-10-17T16:46:51.238102Z","shell.execute_reply.started":"2021-10-17T16:46:48.216865Z","shell.execute_reply":"2021-10-17T16:46:51.237179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stark_df=df[df['character']=='TONY STARK']","metadata":{"id":"EttabM7g1YYl","execution":{"iopub.status.busy":"2021-10-17T16:46:51.240044Z","iopub.execute_input":"2021-10-17T16:46:51.240358Z","iopub.status.idle":"2021-10-17T16:46:51.248903Z","shell.execute_reply.started":"2021-10-17T16:46:51.240301Z","shell.execute_reply":"2021-10-17T16:46:51.248197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stark_df=df[['line','character']]","metadata":{"id":"cUc2y3ST3hnp","execution":{"iopub.status.busy":"2021-10-17T16:46:51.2499Z","iopub.execute_input":"2021-10-17T16:46:51.250166Z","iopub.status.idle":"2021-10-17T16:46:51.260414Z","shell.execute_reply.started":"2021-10-17T16:46:51.250129Z","shell.execute_reply":"2021-10-17T16:46:51.25961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_column(data,column):\n  data[column]=data[column].apply(make_lower_case)\n  # data[column]=data[column].apply(remove_numbers)\n  data[column]=data[column].apply(_removeNonAscii)\n  # data[column]=data[column].apply(remove_stop_words)\n  data[column]=data[column].apply(expand_contractions)\n  # data[column]=data[column].apply(lemmatize)\n  # data[column]=data[column].apply(remove_accented_chars)\n  # data[column]=data[column].apply(remove_html)\n  # data[column]=data[column].apply(remove_punctuation)\n  return data","metadata":{"id":"D_RWm4dt4LJU","execution":{"iopub.status.busy":"2021-10-17T16:46:51.262119Z","iopub.execute_input":"2021-10-17T16:46:51.262412Z","iopub.status.idle":"2021-10-17T16:46:51.270291Z","shell.execute_reply.started":"2021-10-17T16:46:51.262375Z","shell.execute_reply":"2021-10-17T16:46:51.26963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stark_df=preprocess_column(stark_df,'line')","metadata":{"id":"UciKGpMe4NVL","outputId":"029d9cb9-29ca-47c3-8787-b1bec9418df0","execution":{"iopub.status.busy":"2021-10-17T16:46:51.272115Z","iopub.execute_input":"2021-10-17T16:46:51.272433Z","iopub.status.idle":"2021-10-17T16:46:51.573962Z","shell.execute_reply.started":"2021-10-17T16:46:51.272399Z","shell.execute_reply":"2021-10-17T16:46:51.57324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contexted = []\nn = 7\nfor i in range(n, len(stark_df['line'])):\n  row = []\n  prev = i - 1 - n # we additionally subtract 1, so row will contain current response and 7 previous responses  \n  for j in range(i, prev, -1):\n    row.append(stark_df['line'][j])\n  contexted.append(row)\ncolumns = ['response', 'context'] \ncolumns = columns + ['context/'+str(i) for i in range(n-1)]\ncontexted_df = pd.DataFrame.from_records(contexted, columns=columns)\ncontexted_df.head(5)","metadata":{"id":"biiQCFFy1EQ3","outputId":"33d861eb-bb23-4e46-a6f2-7f21a33a7705","execution":{"iopub.status.busy":"2021-10-17T16:46:51.575286Z","iopub.execute_input":"2021-10-17T16:46:51.575572Z","iopub.status.idle":"2021-10-17T16:46:53.006319Z","shell.execute_reply.started":"2021-10-17T16:46:51.575538Z","shell.execute_reply":"2021-10-17T16:46:53.005614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn_df, val_df = train_test_split(contexted_df, test_size = 0.1)","metadata":{"id":"TKoUA6gW1G_L","execution":{"iopub.status.busy":"2021-10-17T16:46:53.00772Z","iopub.execute_input":"2021-10-17T16:46:53.008007Z","iopub.status.idle":"2021-10-17T16:46:53.016809Z","shell.execute_reply.started":"2021-10-17T16:46:53.007971Z","shell.execute_reply":"2021-10-17T16:46:53.015898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def construct_conv(row, tokenizer, eos = True):\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n    conv = flatten(conv)\n    return conv\n\nclass ConversationDataset(Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n\n        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n\n        directory = args.cache_dir\n        cached_features_file = os.path.join(\n            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n        )\n\n        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n            logger.info(\"Loading features from cached file %s\", cached_features_file)\n            with open(cached_features_file, \"rb\") as handle:\n                self.examples = pickle.load(handle)\n        else:\n            logger.info(\"Creating features from dataset file at %s\", directory)\n\n            self.examples = []\n            for _, row in df.iterrows():\n                conv = construct_conv(row, tokenizer)\n                self.examples.append(conv)\n\n            logger.info(\"Saving features into cached file %s\", cached_features_file)\n            with open(cached_features_file, \"wb\") as handle:\n                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, item):\n        return torch.tensor(self.examples[item], dtype=torch.long)\n      \n# Cacheing and storing of data/checkpoints\n\ndef load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\ndef _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n    ordering_and_checkpoint_path = []\n\n    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n\n    for path in glob_checkpoints:\n        if use_mtime:\n            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n        else:\n            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n            if regex_match and regex_match.groups():\n                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n\n    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n    return checkpoints_sorted\n\ndef _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n    if not args.save_total_limit:\n        return\n    if args.save_total_limit <= 0:\n        return\n\n    # Check if we should delete older checkpoint(s)\n    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n    if len(checkpoints_sorted) <= args.save_total_limit:\n        return\n\n    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n    for checkpoint in checkpoints_to_be_deleted:\n        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n        shutil.rmtree(checkpoint)","metadata":{"id":"IvhXFFsB4ki3","execution":{"iopub.status.busy":"2021-10-17T16:46:53.018644Z","iopub.execute_input":"2021-10-17T16:46:53.018981Z","iopub.status.idle":"2021-10-17T16:46:53.040437Z","shell.execute_reply.started":"2021-10-17T16:46:53.018933Z","shell.execute_reply":"2021-10-17T16:46:53.039662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n  \"\"\" Train the model \"\"\"\n  if args.local_rank in [-1, 0]:\n      tb_writer = SummaryWriter()\n\n  args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n\n  def collate(examples: List[torch.Tensor]):\n      if tokenizer._pad_token is None:\n          return pad_sequence(examples, batch_first=True)\n      return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n  train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n  train_dataloader = DataLoader(\n      train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n  )\n\n  if args.max_steps > 0:\n      t_total = args.max_steps\n      args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n  else:\n      t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n  model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n  model.resize_token_embeddings(len(tokenizer))\n  # add_special_tokens_(model, tokenizer)\n\n\n  # Prepare optimizer and schedule (linear warmup and decay)\n  no_decay = [\"bias\", \"LayerNorm.weight\"]\n  optimizer_grouped_parameters = [\n      {\n          \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n          \"weight_decay\": args.weight_decay,\n      },\n      {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n  ]\n  optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n  scheduler = get_linear_schedule_with_warmup(\n      optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n  )\n\n  # Check if saved optimizer or scheduler states exist\n  if (\n      args.model_name_or_path\n      and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n      and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n  ):\n      # Load in optimizer and scheduler states\n      optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n      scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n\n  if args.fp16:\n      try:\n          from apex import amp\n      except ImportError:\n          raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n      model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n  # multi-gpu training (should be after apex fp16 initialization)\n  if args.n_gpu > 1:\n      model = torch.nn.DataParallel(model)\n\n  # Distributed training (should be after apex fp16 initialization)\n  if args.local_rank != -1:\n      model = torch.nn.parallel.DistributedDataParallel(\n          model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n      )\n\n  # Train!\n  logger.info(\"***** Running training *****\")\n  logger.info(\"  Num examples = %d\", len(train_dataset))\n  logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n  logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n  logger.info(\n      \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n      args.train_batch_size\n      * args.gradient_accumulation_steps\n      * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n  )\n  logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n  logger.info(\"  Total optimization steps = %d\", t_total)\n\n  global_step = 0\n  epochs_trained = 0\n  steps_trained_in_current_epoch = 0\n  # Check if continuing training from a checkpoint\n  if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n      try:\n          # set global_step to gobal_step of last saved checkpoint from model path\n          checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n          global_step = int(checkpoint_suffix)\n          epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n          steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n\n          logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n          logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n          logger.info(\"  Continuing training from global step %d\", global_step)\n          logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n      except ValueError:\n          logger.info(\"  Starting fine-tuning.\")\n\n  tr_loss, logging_loss = 0.0, 0.0\n\n  model.zero_grad()\n  train_iterator = trange(\n      epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n  )\n  set_seed(args)  # Added here for reproducibility\n  for _ in train_iterator:\n      epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n      for step, batch in enumerate(epoch_iterator):\n\n          # Skip past any already trained steps if resuming training\n          if steps_trained_in_current_epoch > 0:\n              steps_trained_in_current_epoch -= 1\n              continue\n\n          inputs, labels = (batch, batch)\n          if inputs.shape[1] > 1024: continue\n          inputs = inputs.to(args.device)\n          labels = labels.to(args.device)\n          model.train()\n          outputs = model(inputs, labels=labels)\n          loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n          if args.n_gpu > 1:\n              loss = loss.mean()  # mean() to average on multi-gpu parallel training\n          if args.gradient_accumulation_steps > 1:\n              loss = loss / args.gradient_accumulation_steps\n\n          if args.fp16:\n              with amp.scale_loss(loss, optimizer) as scaled_loss:\n                  scaled_loss.backward()\n          else:\n              loss.backward()\n\n          tr_loss += loss.item()\n          if (step + 1) % args.gradient_accumulation_steps == 0:\n              if args.fp16:\n                  torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n              else:\n                  torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n              optimizer.step()\n              scheduler.step()  # Update learning rate schedule\n              model.zero_grad()\n              global_step += 1\n\n              if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                  # Log metrics\n                  if (\n                      args.local_rank == -1 and args.evaluate_during_training\n                  ):  # Only evaluate when single GPU otherwise metrics may not average well\n                      results = evaluate(args, model, tokenizer)\n                      for key, value in results.items():\n                          tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n                  tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                  tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                  logging_loss = tr_loss\n\n              if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                  checkpoint_prefix = \"checkpoint\"\n                  # Save model checkpoint\n                  output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n                  os.makedirs(output_dir, exist_ok=True)\n                  model_to_save = (\n                      model.module if hasattr(model, \"module\") else model\n                  )  # Take care of distributed/parallel training\n                  model_to_save.save_pretrained(output_dir)\n                  tokenizer.save_pretrained(output_dir)\n\n                  torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                  logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n                  _rotate_checkpoints(args, checkpoint_prefix)\n\n                  torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                  torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n                  logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n\n          if args.max_steps > 0 and global_step > args.max_steps:\n              epoch_iterator.close()\n              break\n      if args.max_steps > 0 and global_step > args.max_steps:\n          train_iterator.close()\n          break\n\n  if args.local_rank in [-1, 0]:\n      tb_writer.close()\n\n  return global_step, tr_loss / global_step\n\n# Evaluation of some model\n\ndef evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n  # Loop to handle MNLI double evaluation (matched, mis-matched)\n  eval_output_dir = args.output_dir\n\n  eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n  os.makedirs(eval_output_dir, exist_ok=True)\n  args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n  # Note that DistributedSampler samples randomly\n\n  def collate(examples: List[torch.Tensor]):\n      if tokenizer._pad_token is None:\n          return pad_sequence(examples, batch_first=True)\n      return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n  eval_sampler = SequentialSampler(eval_dataset)\n  eval_dataloader = DataLoader(\n      eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n  )\n\n  # multi-gpu evaluate\n  if args.n_gpu > 1:\n      model = torch.nn.DataParallel(model)\n\n  # Eval!\n  logger.info(\"***** Running evaluation {} *****\".format(prefix))\n  logger.info(\"  Num examples = %d\", len(eval_dataset))\n  logger.info(\"  Batch size = %d\", args.eval_batch_size)\n  eval_loss = 0.0\n  nb_eval_steps = 0\n  model.eval()\n\n  for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n      inputs, labels = (batch, batch)\n      inputs = inputs.to(args.device)\n      labels = labels.to(args.device)\n\n      with torch.no_grad():\n          outputs = model(inputs, labels=labels)\n          lm_loss = outputs[0]\n          eval_loss += lm_loss.mean().item()\n      nb_eval_steps += 1\n\n  eval_loss = eval_loss / nb_eval_steps\n  perplexity = torch.exp(torch.tensor(eval_loss))\n\n  result = {\"perplexity\": perplexity}\n\n  output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n  with open(output_eval_file, \"w\") as writer:\n      logger.info(\"***** Eval results {} *****\".format(prefix))\n      for key in sorted(result.keys()):\n          logger.info(\"  %s = %s\", key, str(result[key]))\n          writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n  return result","metadata":{"id":"Eu6NRVw04pv-","execution":{"iopub.status.busy":"2021-10-17T16:46:53.04288Z","iopub.execute_input":"2021-10-17T16:46:53.043181Z","iopub.status.idle":"2021-10-17T16:46:53.091481Z","shell.execute_reply.started":"2021-10-17T16:46:53.043156Z","shell.execute_reply":"2021-10-17T16:46:53.090545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main(df_trn, df_val):\n    args = Args()\n\n    if args.should_continue:\n        sorted_checkpoints = _sorted_checkpoints(args)\n        if len(sorted_checkpoints) == 0:\n            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n        else:\n            args.model_name_or_path = sorted_checkpoints[-1]\n\n    if (\n        os.path.exists(args.output_dir)\n        and os.listdir(args.output_dir)\n        and args.do_train\n        and not args.overwrite_output_dir\n        and not args.should_continue\n    ):\n        raise ValueError(\n            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n                args.output_dir\n            )\n        )\n\n    # Setup CUDA, GPU & distributed training\n    device = torch.device(\"cuda\")\n    args.n_gpu = torch.cuda.device_count()\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n    )\n    logger.warning(\n        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n        args.local_rank,\n        device,\n        args.n_gpu,\n        bool(args.local_rank != -1),\n        args.fp16,\n    )\n\n    # Set seed\n    set_seed(args)\n\n    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n    model = AutoModelWithLMHead.from_pretrained(\n        args.model_name_or_path,\n        from_tf=False,\n        config=config,\n        cache_dir=args.cache_dir,\n    )\n    model.to(args.device)\n\n    logger.info(\"Training/evaluation parameters %s\", args)\n\n    # Training\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n\n    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n    if args.do_train:\n        # Create output directory if needed\n        os.makedirs(args.output_dir, exist_ok=True)\n\n        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = (\n            model.module if hasattr(model, \"module\") else model\n        )  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n\n        # Load a trained model and vocabulary that you have fine-tuned\n        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n        model.to(args.device)\n\n    # Evaluation\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = list(\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n            )\n            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n\n            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n            results.update(result)\n\n    return results , model","metadata":{"id":"pTaQem6i43NF","execution":{"iopub.status.busy":"2021-10-17T16:46:53.094019Z","iopub.execute_input":"2021-10-17T16:46:53.094592Z","iopub.status.idle":"2021-10-17T16:46:53.114761Z","shell.execute_reply.started":"2021-10-17T16:46:53.094555Z","shell.execute_reply":"2021-10-17T16:46:53.114036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results, model = main(trn_df, val_df)","metadata":{"id":"69tdK8kG5AW6","outputId":"3b84e423-168e-4eea-a2b7-5bfbe5d123f4","execution":{"iopub.status.busy":"2021-10-17T16:46:53.11625Z","iopub.execute_input":"2021-10-17T16:46:53.11663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's chat for 5 lines\nfor step in range(5):\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    new_user_input_ids = tokenizer.encode(input(\">> User: \") + tokenizer.eos_token, return_tensors='pt').to(device)\n    # print(new_user_input_ids)\n# append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1).to(device) if step > 0 else new_user_input_ids\n# generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(\n        bot_input_ids, max_length=200,\n        pad_token_id=tokenizer.eos_token_id,  \n        no_repeat_ngram_size=3,       \n        do_sample=True, \n        top_k=100, \n        top_p=0.7,\n        temperature = 0.8\n    )\n    \n    # pretty print last ouput tokens from bot\n    print(\"Tony : {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))","metadata":{"id":"4iE8OoIdMQHG","trusted":true},"execution_count":null,"outputs":[]}]}